[WARNING|2024-11-26 22:07:06] logging.py:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.

[INFO|2024-11-26 22:07:06] parser.py:355 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16

[INFO|2024-11-26 22:07:06] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-26 22:07:06] configuration_utils.py:746 >> Model config LlamaConfig {
  "_name_or_path": "/root/autodl-tmp/deepseek-llm-7b-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-26 22:07:06] tokenization_utils_base.py:2209 >> loading file tokenizer.model

[INFO|2024-11-26 22:07:06] tokenization_utils_base.py:2209 >> loading file tokenizer.json

[INFO|2024-11-26 22:07:06] tokenization_utils_base.py:2209 >> loading file added_tokens.json

[INFO|2024-11-26 22:07:06] tokenization_utils_base.py:2209 >> loading file special_tokens_map.json

[INFO|2024-11-26 22:07:06] tokenization_utils_base.py:2209 >> loading file tokenizer_config.json

[INFO|2024-11-26 22:07:06] parser.py:355 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16

[INFO|2024-11-26 22:07:07] tokenization_utils_base.py:2475 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

[INFO|2024-11-26 22:07:07] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-26 22:07:07] configuration_utils.py:746 >> Model config LlamaConfig {
  "_name_or_path": "/root/autodl-tmp/deepseek-llm-7b-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-26 22:07:07] tokenization_utils_base.py:2209 >> loading file tokenizer.model

[INFO|2024-11-26 22:07:07] tokenization_utils_base.py:2209 >> loading file tokenizer.json

[INFO|2024-11-26 22:07:07] tokenization_utils_base.py:2209 >> loading file added_tokens.json

[INFO|2024-11-26 22:07:07] tokenization_utils_base.py:2209 >> loading file special_tokens_map.json

[INFO|2024-11-26 22:07:07] tokenization_utils_base.py:2209 >> loading file tokenizer_config.json

[INFO|2024-11-26 22:07:07] tokenization_utils_base.py:2475 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

[INFO|2024-11-26 22:07:07] logging.py:157 >> Loading dataset lessonPlan_data.json...

[INFO|2024-11-26 22:07:10] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-26 22:07:10] configuration_utils.py:746 >> Model config LlamaConfig {
  "_name_or_path": "/root/autodl-tmp/deepseek-llm-7b-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-26 22:07:10] modeling_utils.py:3934 >> loading weights file /root/autodl-tmp/deepseek-llm-7b-chat/pytorch_model.bin.index.json

[INFO|2024-11-26 22:07:10] modeling_utils.py:1670 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.

[INFO|2024-11-26 22:07:10] configuration_utils.py:1096 >> Generate config GenerationConfig {
  "bos_token_id": 100000,
  "eos_token_id": 100001
}


[INFO|2024-11-26 22:07:13] modeling_utils.py:4800 >> All model checkpoint weights were used when initializing LlamaForCausalLM.


[INFO|2024-11-26 22:07:13] modeling_utils.py:4808 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/autodl-tmp/deepseek-llm-7b-chat.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

[INFO|2024-11-26 22:07:13] configuration_utils.py:1049 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/generation_config.json

[INFO|2024-11-26 22:07:13] configuration_utils.py:1096 >> Generate config GenerationConfig {
  "bos_token_id": 100000,
  "do_sample": true,
  "eos_token_id": 100001,
  "temperature": 0.7,
  "top_p": 0.95
}


[INFO|2024-11-26 22:07:13] logging.py:157 >> Gradient checkpointing enabled.

[INFO|2024-11-26 22:07:13] logging.py:157 >> Using torch SDPA for faster training and inference.

[INFO|2024-11-26 22:07:13] logging.py:157 >> Upcasting trainable params to float32.

[INFO|2024-11-26 22:07:13] logging.py:157 >> Fine-tuning method: LoRA

[INFO|2024-11-26 22:07:13] logging.py:157 >> Found linear modules: v_proj,o_proj,k_proj,up_proj,down_proj,q_proj,gate_proj

[INFO|2024-11-26 22:07:14] logging.py:157 >> trainable params: 18,739,200 || all params: 6,929,104,896 || trainable%: 0.2704

[INFO|2024-11-26 22:07:14] trainer.py:698 >> Using auto half precision backend

[INFO|2024-11-26 22:07:15] trainer.py:2313 >> ***** Running training *****

[INFO|2024-11-26 22:07:15] trainer.py:2314 >>   Num examples = 2,182

[INFO|2024-11-26 22:07:15] trainer.py:2315 >>   Num Epochs = 1,000

[INFO|2024-11-26 22:07:15] trainer.py:2316 >>   Instantaneous batch size per device = 1

[INFO|2024-11-26 22:07:15] trainer.py:2319 >>   Total train batch size (w. parallel, distributed & accumulation) = 128

[INFO|2024-11-26 22:07:15] trainer.py:2320 >>   Gradient Accumulation steps = 64

[INFO|2024-11-26 22:07:15] trainer.py:2321 >>   Total optimization steps = 17,000

[INFO|2024-11-26 22:07:15] trainer.py:2322 >>   Number of trainable parameters = 18,739,200

[INFO|2024-11-26 22:09:01] logging.py:157 >> {'loss': 2.2818, 'learning_rate': 2.5000e-06, 'epoch': 0.29}

[INFO|2024-11-26 22:10:54] logging.py:157 >> {'loss': 2.2647, 'learning_rate': 5.0000e-06, 'epoch': 0.59}

[INFO|2024-11-26 22:12:37] logging.py:157 >> {'loss': 2.3045, 'learning_rate': 7.5000e-06, 'epoch': 0.88}

[INFO|2024-11-26 22:14:22] logging.py:157 >> {'loss': 2.8274, 'learning_rate': 1.0000e-05, 'epoch': 1.17}

[INFO|2024-11-26 22:16:11] logging.py:157 >> {'loss': 2.3486, 'learning_rate': 1.2500e-05, 'epoch': 1.47}

[INFO|2024-11-26 22:18:02] logging.py:157 >> {'loss': 2.2083, 'learning_rate': 1.5000e-05, 'epoch': 1.76}

[INFO|2024-11-26 22:19:43] logging.py:157 >> {'loss': 2.8014, 'learning_rate': 1.7500e-05, 'epoch': 2.05}

[INFO|2024-11-26 22:21:31] logging.py:157 >> {'loss': 2.1802, 'learning_rate': 2.0000e-05, 'epoch': 2.35}

[INFO|2024-11-26 22:23:18] logging.py:157 >> {'loss': 2.2209, 'learning_rate': 2.2500e-05, 'epoch': 2.64}

[INFO|2024-11-26 22:25:01] logging.py:157 >> {'loss': 2.0920, 'learning_rate': 2.5000e-05, 'epoch': 2.93}

[INFO|2024-11-26 22:26:49] logging.py:157 >> {'loss': 2.6093, 'learning_rate': 2.7500e-05, 'epoch': 3.23}

[INFO|2024-11-26 22:28:42] logging.py:157 >> {'loss': 2.1144, 'learning_rate': 3.0000e-05, 'epoch': 3.52}

[INFO|2024-11-26 22:30:28] logging.py:157 >> {'loss': 2.1031, 'learning_rate': 3.2500e-05, 'epoch': 3.81}

[INFO|2024-11-26 22:32:13] logging.py:157 >> {'loss': 2.5317, 'learning_rate': 3.5000e-05, 'epoch': 4.11}

[INFO|2024-11-26 22:34:00] logging.py:157 >> {'loss': 2.0181, 'learning_rate': 3.7500e-05, 'epoch': 4.40}

[INFO|2024-11-26 22:35:45] logging.py:157 >> {'loss': 2.0393, 'learning_rate': 4.0000e-05, 'epoch': 4.69}

[INFO|2024-11-26 22:37:26] logging.py:157 >> {'loss': 2.0459, 'learning_rate': 4.2500e-05, 'epoch': 4.99}

[INFO|2024-11-26 22:39:10] logging.py:157 >> {'loss': 2.4003, 'learning_rate': 4.5000e-05, 'epoch': 5.28}

[INFO|2024-11-26 22:40:57] logging.py:157 >> {'loss': 1.9986, 'learning_rate': 4.7500e-05, 'epoch': 5.57}

[INFO|2024-11-26 22:42:47] logging.py:157 >> {'loss': 1.9937, 'learning_rate': 5.0000e-05, 'epoch': 5.87}

[INFO|2024-11-26 22:42:47] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-26 22:42:47] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-26 22:42:47] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-26 22:43:00] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-100

[INFO|2024-11-26 22:43:00] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-26 22:43:00] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-26 22:43:00] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-100/tokenizer_config.json

[INFO|2024-11-26 22:43:00] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-100/special_tokens_map.json

[INFO|2024-11-26 22:44:52] logging.py:157 >> {'loss': 2.6840, 'learning_rate': 5.0000e-05, 'epoch': 6.16}

[INFO|2024-11-26 22:46:44] logging.py:157 >> {'loss': 1.9705, 'learning_rate': 5.0000e-05, 'epoch': 6.45}

[INFO|2024-11-26 22:48:35] logging.py:157 >> {'loss': 1.9779, 'learning_rate': 5.0000e-05, 'epoch': 6.75}

[INFO|2024-11-26 22:50:25] logging.py:157 >> {'loss': 2.2567, 'learning_rate': 5.0000e-05, 'epoch': 7.04}

[INFO|2024-11-26 22:52:11] logging.py:157 >> {'loss': 1.9427, 'learning_rate': 5.0000e-05, 'epoch': 7.33}

[INFO|2024-11-26 22:54:01] logging.py:157 >> {'loss': 1.8640, 'learning_rate': 5.0000e-05, 'epoch': 7.63}

[INFO|2024-11-26 22:55:53] logging.py:157 >> {'loss': 1.9330, 'learning_rate': 4.9999e-05, 'epoch': 7.92}

[INFO|2024-11-26 22:57:38] logging.py:157 >> {'loss': 2.3341, 'learning_rate': 4.9999e-05, 'epoch': 8.21}

[INFO|2024-11-26 22:59:25] logging.py:157 >> {'loss': 1.8886, 'learning_rate': 4.9999e-05, 'epoch': 8.51}

[INFO|2024-11-26 23:01:20] logging.py:157 >> {'loss': 1.8426, 'learning_rate': 4.9999e-05, 'epoch': 8.80}

[INFO|2024-11-26 23:03:10] logging.py:157 >> {'loss': 2.3015, 'learning_rate': 4.9999e-05, 'epoch': 9.09}

[INFO|2024-11-26 23:05:00] logging.py:157 >> {'loss': 1.8109, 'learning_rate': 4.9998e-05, 'epoch': 9.39}

[INFO|2024-11-26 23:06:47] logging.py:157 >> {'loss': 1.8860, 'learning_rate': 4.9998e-05, 'epoch': 9.68}

[INFO|2024-11-26 23:08:26] logging.py:157 >> {'loss': 1.8606, 'learning_rate': 4.9998e-05, 'epoch': 9.97}

[INFO|2024-11-26 23:10:15] logging.py:157 >> {'loss': 2.1369, 'learning_rate': 4.9998e-05, 'epoch': 10.27}

[INFO|2024-11-26 23:12:02] logging.py:157 >> {'loss': 1.8569, 'learning_rate': 4.9997e-05, 'epoch': 10.56}

[INFO|2024-11-26 23:13:47] logging.py:157 >> {'loss': 1.8509, 'learning_rate': 4.9997e-05, 'epoch': 10.85}

[INFO|2024-11-26 23:15:37] logging.py:157 >> {'loss': 2.1505, 'learning_rate': 4.9997e-05, 'epoch': 11.15}

[INFO|2024-11-26 23:17:38] logging.py:157 >> {'loss': 1.7708, 'learning_rate': 4.9996e-05, 'epoch': 11.44}

[INFO|2024-11-26 23:19:31] logging.py:157 >> {'loss': 1.8334, 'learning_rate': 4.9996e-05, 'epoch': 11.73}

[INFO|2024-11-26 23:19:31] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-26 23:19:31] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-26 23:19:31] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-26 23:19:42] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-200

[INFO|2024-11-26 23:19:42] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-26 23:19:42] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-26 23:19:43] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-200/tokenizer_config.json

[INFO|2024-11-26 23:19:43] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-200/special_tokens_map.json

[INFO|2024-11-26 23:21:46] logging.py:157 >> {'loss': 2.0073, 'learning_rate': 4.9995e-05, 'epoch': 12.03}

[INFO|2024-11-26 23:23:36] logging.py:157 >> {'loss': 1.8143, 'learning_rate': 4.9995e-05, 'epoch': 12.32}

[INFO|2024-11-26 23:25:21] logging.py:157 >> {'loss': 1.7037, 'learning_rate': 4.9994e-05, 'epoch': 12.61}

[INFO|2024-11-26 23:27:07] logging.py:157 >> {'loss': 1.7488, 'learning_rate': 4.9994e-05, 'epoch': 12.91}

[INFO|2024-11-26 23:28:57] logging.py:157 >> {'loss': 2.0890, 'learning_rate': 4.9993e-05, 'epoch': 13.20}

[INFO|2024-11-26 23:30:42] logging.py:157 >> {'loss': 1.7161, 'learning_rate': 4.9993e-05, 'epoch': 13.49}

[INFO|2024-11-26 23:32:29] logging.py:157 >> {'loss': 1.7353, 'learning_rate': 4.9992e-05, 'epoch': 13.79}

[INFO|2024-11-26 23:34:10] logging.py:157 >> {'loss': 1.9326, 'learning_rate': 4.9992e-05, 'epoch': 14.08}

[INFO|2024-11-26 23:35:53] logging.py:157 >> {'loss': 1.6297, 'learning_rate': 4.9991e-05, 'epoch': 14.37}

[INFO|2024-11-26 23:37:43] logging.py:157 >> {'loss': 1.6449, 'learning_rate': 4.9990e-05, 'epoch': 14.67}

[INFO|2024-11-26 23:39:31] logging.py:157 >> {'loss': 1.6916, 'learning_rate': 4.9990e-05, 'epoch': 14.96}

[INFO|2024-11-26 23:41:16] logging.py:157 >> {'loss': 1.9382, 'learning_rate': 4.9989e-05, 'epoch': 15.25}

[INFO|2024-11-26 23:43:00] logging.py:157 >> {'loss': 1.6538, 'learning_rate': 4.9988e-05, 'epoch': 15.55}

[INFO|2024-11-26 23:44:43] logging.py:157 >> {'loss': 1.5764, 'learning_rate': 4.9988e-05, 'epoch': 15.84}

[INFO|2024-11-26 23:46:27] logging.py:157 >> {'loss': 1.9426, 'learning_rate': 4.9987e-05, 'epoch': 16.13}

[INFO|2024-11-26 23:48:14] logging.py:157 >> {'loss': 1.4849, 'learning_rate': 4.9986e-05, 'epoch': 16.43}

[INFO|2024-11-26 23:49:54] logging.py:157 >> {'loss': 1.4831, 'learning_rate': 4.9985e-05, 'epoch': 16.72}

[INFO|2024-11-26 23:51:39] logging.py:157 >> {'loss': 1.8342, 'learning_rate': 4.9984e-05, 'epoch': 17.01}

[INFO|2024-11-26 23:53:20] logging.py:157 >> {'loss': 1.4492, 'learning_rate': 4.9984e-05, 'epoch': 17.31}

[INFO|2024-11-26 23:55:07] logging.py:157 >> {'loss': 1.4419, 'learning_rate': 4.9983e-05, 'epoch': 17.60}

[INFO|2024-11-26 23:55:07] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-26 23:55:07] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-26 23:55:07] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-26 23:55:18] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-300

[INFO|2024-11-26 23:55:18] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-26 23:55:18] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-26 23:55:18] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-300/tokenizer_config.json

[INFO|2024-11-26 23:55:18] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-300/special_tokens_map.json

[INFO|2024-11-26 23:57:07] logging.py:157 >> {'loss': 1.5033, 'learning_rate': 4.9982e-05, 'epoch': 17.89}

[INFO|2024-11-26 23:58:58] logging.py:157 >> {'loss': 1.8666, 'learning_rate': 4.9981e-05, 'epoch': 18.19}

[INFO|2024-11-27 00:00:43] logging.py:157 >> {'loss': 1.4418, 'learning_rate': 4.9980e-05, 'epoch': 18.48}

[INFO|2024-11-27 00:02:28] logging.py:157 >> {'loss': 1.3691, 'learning_rate': 4.9979e-05, 'epoch': 18.77}

[INFO|2024-11-27 00:04:12] logging.py:157 >> {'loss': 1.5875, 'learning_rate': 4.9978e-05, 'epoch': 19.07}

[INFO|2024-11-27 00:06:00] logging.py:157 >> {'loss': 1.3211, 'learning_rate': 4.9977e-05, 'epoch': 19.36}

[INFO|2024-11-27 00:07:47] logging.py:157 >> {'loss': 1.3478, 'learning_rate': 4.9976e-05, 'epoch': 19.65}

[INFO|2024-11-27 00:09:42] logging.py:157 >> {'loss': 1.3587, 'learning_rate': 4.9975e-05, 'epoch': 19.95}

[INFO|2024-11-27 00:11:22] logging.py:157 >> {'loss': 1.5579, 'learning_rate': 4.9974e-05, 'epoch': 20.24}

[INFO|2024-11-27 00:13:05] logging.py:157 >> {'loss': 1.2776, 'learning_rate': 4.9973e-05, 'epoch': 20.53}

[INFO|2024-11-27 00:14:50] logging.py:157 >> {'loss': 1.2463, 'learning_rate': 4.9972e-05, 'epoch': 20.82}

[INFO|2024-11-27 00:16:34] logging.py:157 >> {'loss': 1.5756, 'learning_rate': 4.9971e-05, 'epoch': 21.12}

[INFO|2024-11-27 00:18:21] logging.py:157 >> {'loss': 1.1998, 'learning_rate': 4.9970e-05, 'epoch': 21.41}

[INFO|2024-11-27 00:20:15] logging.py:157 >> {'loss': 1.1980, 'learning_rate': 4.9969e-05, 'epoch': 21.70}

[INFO|2024-11-27 00:22:11] logging.py:157 >> {'loss': 1.3544, 'learning_rate': 4.9967e-05, 'epoch': 22.00}

[INFO|2024-11-27 00:24:01] logging.py:157 >> {'loss': 1.1666, 'learning_rate': 4.9966e-05, 'epoch': 22.29}

[INFO|2024-11-27 00:25:46] logging.py:157 >> {'loss': 1.1233, 'learning_rate': 4.9965e-05, 'epoch': 22.58}

[INFO|2024-11-27 00:27:22] logging.py:157 >> {'loss': 1.2169, 'learning_rate': 4.9964e-05, 'epoch': 22.88}

[INFO|2024-11-27 00:29:11] logging.py:157 >> {'loss': 1.2407, 'learning_rate': 4.9962e-05, 'epoch': 23.17}

[INFO|2024-11-27 00:30:53] logging.py:157 >> {'loss': 1.0530, 'learning_rate': 4.9961e-05, 'epoch': 23.46}

[INFO|2024-11-27 00:30:53] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 00:30:53] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 00:30:53] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-27 00:31:05] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-400

[INFO|2024-11-27 00:31:05] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 00:31:05] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 00:31:05] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-400/tokenizer_config.json

[INFO|2024-11-27 00:31:05] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-400/special_tokens_map.json

[INFO|2024-11-27 00:32:51] logging.py:157 >> {'loss': 1.0625, 'learning_rate': 4.9960e-05, 'epoch': 23.76}

[INFO|2024-11-27 00:34:42] logging.py:157 >> {'loss': 1.2546, 'learning_rate': 4.9959e-05, 'epoch': 24.05}

[INFO|2024-11-27 00:36:28] logging.py:157 >> {'loss': 1.0136, 'learning_rate': 4.9957e-05, 'epoch': 24.34}

[INFO|2024-11-27 00:38:15] logging.py:157 >> {'loss': 0.9918, 'learning_rate': 4.9956e-05, 'epoch': 24.64}

[INFO|2024-11-27 00:40:01] logging.py:157 >> {'loss': 1.0520, 'learning_rate': 4.9954e-05, 'epoch': 24.93}

[INFO|2024-11-27 00:41:54] logging.py:157 >> {'loss': 1.0494, 'learning_rate': 4.9953e-05, 'epoch': 25.22}

[INFO|2024-11-27 00:43:42] logging.py:157 >> {'loss': 1.0234, 'learning_rate': 4.9952e-05, 'epoch': 25.52}

[INFO|2024-11-27 00:45:33] logging.py:157 >> {'loss': 0.9266, 'learning_rate': 4.9950e-05, 'epoch': 25.81}

[INFO|2024-11-27 00:47:22] logging.py:157 >> {'loss': 1.0746, 'learning_rate': 4.9949e-05, 'epoch': 26.10}

[INFO|2024-11-27 00:49:09] logging.py:157 >> {'loss': 0.8861, 'learning_rate': 4.9947e-05, 'epoch': 26.40}

[INFO|2024-11-27 00:50:56] logging.py:157 >> {'loss': 0.8995, 'learning_rate': 4.9946e-05, 'epoch': 26.69}

[INFO|2024-11-27 00:52:56] logging.py:157 >> {'loss': 0.9603, 'learning_rate': 4.9944e-05, 'epoch': 26.98}

[INFO|2024-11-27 00:54:44] logging.py:157 >> {'loss': 1.1393, 'learning_rate': 4.9942e-05, 'epoch': 27.28}

[INFO|2024-11-27 00:56:31] logging.py:157 >> {'loss': 0.8489, 'learning_rate': 4.9941e-05, 'epoch': 27.57}

[INFO|2024-11-27 00:58:25] logging.py:157 >> {'loss': 0.8624, 'learning_rate': 4.9939e-05, 'epoch': 27.86}

[INFO|2024-11-27 01:00:19] logging.py:157 >> {'loss': 1.0073, 'learning_rate': 4.9938e-05, 'epoch': 28.16}

[INFO|2024-11-27 01:02:09] logging.py:157 >> {'loss': 0.7706, 'learning_rate': 4.9936e-05, 'epoch': 28.45}

[INFO|2024-11-27 01:03:53] logging.py:157 >> {'loss': 0.8315, 'learning_rate': 4.9934e-05, 'epoch': 28.74}

[INFO|2024-11-27 01:05:36] logging.py:157 >> {'loss': 0.9148, 'learning_rate': 4.9933e-05, 'epoch': 29.04}

[INFO|2024-11-27 01:07:29] logging.py:157 >> {'loss': 0.7372, 'learning_rate': 4.9931e-05, 'epoch': 29.33}

[INFO|2024-11-27 01:07:29] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 01:07:29] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 01:07:29] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-27 01:07:42] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-500

[INFO|2024-11-27 01:07:42] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 01:07:42] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 01:07:43] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-500/tokenizer_config.json

[INFO|2024-11-27 01:07:43] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-500/special_tokens_map.json

[INFO|2024-11-27 01:09:30] logging.py:157 >> {'loss': 0.8077, 'learning_rate': 4.9929e-05, 'epoch': 29.62}

[INFO|2024-11-27 01:11:13] logging.py:157 >> {'loss': 0.7593, 'learning_rate': 4.9927e-05, 'epoch': 29.92}

[INFO|2024-11-27 01:13:02] logging.py:157 >> {'loss': 0.8972, 'learning_rate': 4.9926e-05, 'epoch': 30.21}

[INFO|2024-11-27 01:14:54] logging.py:157 >> {'loss': 0.7791, 'learning_rate': 4.9924e-05, 'epoch': 30.50}

[INFO|2024-11-27 01:16:41] logging.py:157 >> {'loss': 0.6931, 'learning_rate': 4.9922e-05, 'epoch': 30.80}

[INFO|2024-11-27 01:18:27] logging.py:157 >> {'loss': 0.8658, 'learning_rate': 4.9920e-05, 'epoch': 31.09}

[INFO|2024-11-27 01:20:09] logging.py:157 >> {'loss': 0.6895, 'learning_rate': 4.9918e-05, 'epoch': 31.38}

[INFO|2024-11-27 01:21:59] logging.py:157 >> {'loss': 0.6873, 'learning_rate': 4.9916e-05, 'epoch': 31.68}

[INFO|2024-11-27 01:23:49] logging.py:157 >> {'loss': 0.6843, 'learning_rate': 4.9915e-05, 'epoch': 31.97}

[INFO|2024-11-27 01:25:38] logging.py:157 >> {'loss': 0.8666, 'learning_rate': 4.9913e-05, 'epoch': 32.26}

[INFO|2024-11-27 01:27:21] logging.py:157 >> {'loss': 0.6145, 'learning_rate': 4.9911e-05, 'epoch': 32.56}

[INFO|2024-11-27 01:29:11] logging.py:157 >> {'loss': 0.6698, 'learning_rate': 4.9909e-05, 'epoch': 32.85}

[INFO|2024-11-27 01:30:57] logging.py:157 >> {'loss': 0.7219, 'learning_rate': 4.9907e-05, 'epoch': 33.14}

[INFO|2024-11-27 01:32:39] logging.py:157 >> {'loss': 0.5586, 'learning_rate': 4.9905e-05, 'epoch': 33.44}

[INFO|2024-11-27 01:34:25] logging.py:157 >> {'loss': 0.6260, 'learning_rate': 4.9903e-05, 'epoch': 33.73}

[INFO|2024-11-27 01:36:12] logging.py:157 >> {'loss': 0.7748, 'learning_rate': 4.9901e-05, 'epoch': 34.02}

[INFO|2024-11-27 01:37:58] logging.py:157 >> {'loss': 0.5394, 'learning_rate': 4.9898e-05, 'epoch': 34.32}

[INFO|2024-11-27 01:39:43] logging.py:157 >> {'loss': 0.5847, 'learning_rate': 4.9896e-05, 'epoch': 34.61}

[INFO|2024-11-27 01:41:26] logging.py:157 >> {'loss': 0.5741, 'learning_rate': 4.9894e-05, 'epoch': 34.90}

[INFO|2024-11-27 01:43:12] logging.py:157 >> {'loss': 0.7273, 'learning_rate': 4.9892e-05, 'epoch': 35.20}

[INFO|2024-11-27 01:43:12] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 01:43:12] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 01:43:12] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-27 01:43:23] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-600

[INFO|2024-11-27 01:43:23] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 01:43:23] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 01:43:23] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-600/tokenizer_config.json

[INFO|2024-11-27 01:43:23] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-600/special_tokens_map.json

[INFO|2024-11-27 01:45:07] logging.py:157 >> {'loss': 0.5251, 'learning_rate': 4.9890e-05, 'epoch': 35.49}

[INFO|2024-11-27 01:47:05] logging.py:157 >> {'loss': 0.5521, 'learning_rate': 4.9888e-05, 'epoch': 35.78}

[INFO|2024-11-27 01:48:47] logging.py:157 >> {'loss': 0.6675, 'learning_rate': 4.9886e-05, 'epoch': 36.08}

[INFO|2024-11-27 01:50:34] logging.py:157 >> {'loss': 0.4661, 'learning_rate': 4.9883e-05, 'epoch': 36.37}

[INFO|2024-11-27 01:52:17] logging.py:157 >> {'loss': 0.4719, 'learning_rate': 4.9881e-05, 'epoch': 36.66}

[INFO|2024-11-27 01:54:03] logging.py:157 >> {'loss': 0.5684, 'learning_rate': 4.9879e-05, 'epoch': 36.96}

[INFO|2024-11-27 01:55:51] logging.py:157 >> {'loss': 0.6043, 'learning_rate': 4.9876e-05, 'epoch': 37.25}

[INFO|2024-11-27 01:57:36] logging.py:157 >> {'loss': 0.4673, 'learning_rate': 4.9874e-05, 'epoch': 37.54}

[INFO|2024-11-27 01:59:24] logging.py:157 >> {'loss': 0.4428, 'learning_rate': 4.9872e-05, 'epoch': 37.84}

[INFO|2024-11-27 02:01:12] logging.py:157 >> {'loss': 0.5620, 'learning_rate': 4.9869e-05, 'epoch': 38.13}

[INFO|2024-11-27 02:03:01] logging.py:157 >> {'loss': 0.4020, 'learning_rate': 4.9867e-05, 'epoch': 38.42}

[INFO|2024-11-27 02:04:49] logging.py:157 >> {'loss': 0.4451, 'learning_rate': 4.9865e-05, 'epoch': 38.72}

[INFO|2024-11-27 02:06:28] logging.py:157 >> {'loss': 0.5468, 'learning_rate': 4.9862e-05, 'epoch': 39.01}

[INFO|2024-11-27 02:08:21] logging.py:157 >> {'loss': 0.4357, 'learning_rate': 4.9860e-05, 'epoch': 39.30}

[INFO|2024-11-27 02:10:12] logging.py:157 >> {'loss': 0.4243, 'learning_rate': 4.9857e-05, 'epoch': 39.60}

[INFO|2024-11-27 02:11:58] logging.py:157 >> {'loss': 0.3890, 'learning_rate': 4.9855e-05, 'epoch': 39.89}

[INFO|2024-11-27 02:13:39] logging.py:157 >> {'loss': 0.4014, 'learning_rate': 4.9852e-05, 'epoch': 40.18}

[INFO|2024-11-27 02:15:24] logging.py:157 >> {'loss': 0.3767, 'learning_rate': 4.9850e-05, 'epoch': 40.48}

[INFO|2024-11-27 02:17:12] logging.py:157 >> {'loss': 0.3992, 'learning_rate': 4.9847e-05, 'epoch': 40.77}

[INFO|2024-11-27 02:18:57] logging.py:157 >> {'loss': 0.4321, 'learning_rate': 4.9845e-05, 'epoch': 41.06}

[INFO|2024-11-27 02:18:57] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 02:18:57] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 02:18:57] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-27 02:19:08] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-700

[INFO|2024-11-27 02:19:08] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 02:19:08] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 02:19:08] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-700/tokenizer_config.json

[INFO|2024-11-27 02:19:08] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-700/special_tokens_map.json

[INFO|2024-11-27 02:21:05] logging.py:157 >> {'loss': 0.3745, 'learning_rate': 4.9842e-05, 'epoch': 41.36}

[INFO|2024-11-27 02:22:50] logging.py:157 >> {'loss': 0.3714, 'learning_rate': 4.9839e-05, 'epoch': 41.65}

[INFO|2024-11-27 02:24:34] logging.py:157 >> {'loss': 0.3415, 'learning_rate': 4.9837e-05, 'epoch': 41.94}

[INFO|2024-11-27 02:26:24] logging.py:157 >> {'loss': 0.4726, 'learning_rate': 4.9834e-05, 'epoch': 42.24}

[INFO|2024-11-27 02:28:14] logging.py:157 >> {'loss': 0.3160, 'learning_rate': 4.9831e-05, 'epoch': 42.53}

[INFO|2024-11-27 02:30:02] logging.py:157 >> {'loss': 0.3479, 'learning_rate': 4.9829e-05, 'epoch': 42.82}

[INFO|2024-11-27 02:31:54] logging.py:157 >> {'loss': 0.3497, 'learning_rate': 4.9826e-05, 'epoch': 43.12}

[INFO|2024-11-27 02:33:45] logging.py:157 >> {'loss': 0.2879, 'learning_rate': 4.9823e-05, 'epoch': 43.41}

[INFO|2024-11-27 02:35:37] logging.py:157 >> {'loss': 0.3204, 'learning_rate': 4.9821e-05, 'epoch': 43.70}

[INFO|2024-11-27 02:37:23] logging.py:157 >> {'loss': 0.3203, 'learning_rate': 4.9818e-05, 'epoch': 44.00}

[INFO|2024-11-27 02:39:10] logging.py:157 >> {'loss': 0.3022, 'learning_rate': 4.9815e-05, 'epoch': 44.29}

[INFO|2024-11-27 02:40:57] logging.py:157 >> {'loss': 0.2716, 'learning_rate': 4.9812e-05, 'epoch': 44.58}

[INFO|2024-11-27 02:42:40] logging.py:157 >> {'loss': 0.3162, 'learning_rate': 4.9809e-05, 'epoch': 44.88}

[INFO|2024-11-27 02:44:18] logging.py:157 >> {'loss': 0.3065, 'learning_rate': 4.9806e-05, 'epoch': 45.17}

[INFO|2024-11-27 02:46:01] logging.py:157 >> {'loss': 0.2765, 'learning_rate': 4.9803e-05, 'epoch': 45.46}

[INFO|2024-11-27 02:47:43] logging.py:157 >> {'loss': 0.2778, 'learning_rate': 4.9801e-05, 'epoch': 45.76}

[INFO|2024-11-27 02:49:26] logging.py:157 >> {'loss': 0.2780, 'learning_rate': 4.9798e-05, 'epoch': 46.05}

[INFO|2024-11-27 02:51:14] logging.py:157 >> {'loss': 0.2292, 'learning_rate': 4.9795e-05, 'epoch': 46.34}

[INFO|2024-11-27 02:53:04] logging.py:157 >> {'loss': 0.2647, 'learning_rate': 4.9792e-05, 'epoch': 46.64}

[INFO|2024-11-27 02:54:56] logging.py:157 >> {'loss': 0.2520, 'learning_rate': 4.9789e-05, 'epoch': 46.93}

[INFO|2024-11-27 02:54:56] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 02:54:56] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 02:54:56] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-27 02:55:07] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-800

[INFO|2024-11-27 02:55:07] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 02:55:07] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 02:55:07] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-800/tokenizer_config.json

[INFO|2024-11-27 02:55:07] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-800/special_tokens_map.json

[INFO|2024-11-27 02:56:57] logging.py:157 >> {'loss': 0.2813, 'learning_rate': 4.9786e-05, 'epoch': 47.22}

[INFO|2024-11-27 02:58:41] logging.py:157 >> {'loss': 0.2194, 'learning_rate': 4.9783e-05, 'epoch': 47.52}

[INFO|2024-11-27 03:00:32] logging.py:157 >> {'loss': 0.2478, 'learning_rate': 4.9779e-05, 'epoch': 47.81}

[INFO|2024-11-27 03:02:18] logging.py:157 >> {'loss': 0.2461, 'learning_rate': 4.9776e-05, 'epoch': 48.10}

[INFO|2024-11-27 03:04:08] logging.py:157 >> {'loss': 0.1990, 'learning_rate': 4.9773e-05, 'epoch': 48.40}

[INFO|2024-11-27 03:05:51] logging.py:157 >> {'loss': 0.2095, 'learning_rate': 4.9770e-05, 'epoch': 48.69}

[INFO|2024-11-27 03:07:38] logging.py:157 >> {'loss': 0.2257, 'learning_rate': 4.9767e-05, 'epoch': 48.98}

[INFO|2024-11-27 03:09:20] logging.py:157 >> {'loss': 0.2446, 'learning_rate': 4.9764e-05, 'epoch': 49.28}

[INFO|2024-11-27 03:11:11] logging.py:157 >> {'loss': 0.2106, 'learning_rate': 4.9761e-05, 'epoch': 49.57}

[INFO|2024-11-27 03:12:56] logging.py:157 >> {'loss': 0.1701, 'learning_rate': 4.9757e-05, 'epoch': 49.86}

[INFO|2024-11-27 03:14:38] logging.py:157 >> {'loss': 0.2330, 'learning_rate': 4.9754e-05, 'epoch': 50.16}

[INFO|2024-11-27 03:16:20] logging.py:157 >> {'loss': 0.1585, 'learning_rate': 4.9751e-05, 'epoch': 50.45}

[INFO|2024-11-27 03:18:07] logging.py:157 >> {'loss': 0.1971, 'learning_rate': 4.9748e-05, 'epoch': 50.74}

[INFO|2024-11-27 03:19:56] logging.py:157 >> {'loss': 0.2221, 'learning_rate': 4.9744e-05, 'epoch': 51.04}

[INFO|2024-11-27 03:21:38] logging.py:157 >> {'loss': 0.1660, 'learning_rate': 4.9741e-05, 'epoch': 51.33}

[INFO|2024-11-27 03:23:21] logging.py:157 >> {'loss': 0.1686, 'learning_rate': 4.9738e-05, 'epoch': 51.62}

[INFO|2024-11-27 03:25:06] logging.py:157 >> {'loss': 0.1688, 'learning_rate': 4.9734e-05, 'epoch': 51.92}

[INFO|2024-11-27 03:26:48] logging.py:157 >> {'loss': 0.1830, 'learning_rate': 4.9731e-05, 'epoch': 52.21}

[INFO|2024-11-27 03:28:37] logging.py:157 >> {'loss': 0.1530, 'learning_rate': 4.9727e-05, 'epoch': 52.50}

[INFO|2024-11-27 03:30:17] logging.py:157 >> {'loss': 0.1523, 'learning_rate': 4.9724e-05, 'epoch': 52.80}

[INFO|2024-11-27 03:30:17] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 03:30:17] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 03:30:17] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-27 03:30:28] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-900

[INFO|2024-11-27 03:30:28] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 03:30:28] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 03:30:28] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-900/tokenizer_config.json

[INFO|2024-11-27 03:30:28] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-900/special_tokens_map.json

[INFO|2024-11-27 03:32:09] logging.py:157 >> {'loss': 0.1988, 'learning_rate': 4.9721e-05, 'epoch': 53.09}

[INFO|2024-11-27 03:33:54] logging.py:157 >> {'loss': 0.1473, 'learning_rate': 4.9717e-05, 'epoch': 53.38}

[INFO|2024-11-27 03:35:41] logging.py:157 >> {'loss': 0.1340, 'learning_rate': 4.9714e-05, 'epoch': 53.68}

[INFO|2024-11-27 03:37:22] logging.py:157 >> {'loss': 0.1475, 'learning_rate': 4.9710e-05, 'epoch': 53.97}

[INFO|2024-11-27 03:39:02] logging.py:157 >> {'loss': 0.1536, 'learning_rate': 4.9707e-05, 'epoch': 54.26}

[INFO|2024-11-27 03:40:48] logging.py:157 >> {'loss': 0.1404, 'learning_rate': 4.9703e-05, 'epoch': 54.56}

[INFO|2024-11-27 03:42:29] logging.py:157 >> {'loss': 0.1169, 'learning_rate': 4.9699e-05, 'epoch': 54.85}

[INFO|2024-11-27 03:44:10] logging.py:157 >> {'loss': 0.1648, 'learning_rate': 4.9696e-05, 'epoch': 55.14}

[INFO|2024-11-27 03:45:52] logging.py:157 >> {'loss': 0.1013, 'learning_rate': 4.9692e-05, 'epoch': 55.44}

[INFO|2024-11-27 03:47:34] logging.py:157 >> {'loss': 0.1325, 'learning_rate': 4.9689e-05, 'epoch': 55.73}

[INFO|2024-11-27 03:49:16] logging.py:157 >> {'loss': 0.1447, 'learning_rate': 4.9685e-05, 'epoch': 56.02}

[INFO|2024-11-27 03:50:59] logging.py:157 >> {'loss': 0.1091, 'learning_rate': 4.9681e-05, 'epoch': 56.32}

[INFO|2024-11-27 03:52:43] logging.py:157 >> {'loss': 0.1100, 'learning_rate': 4.9677e-05, 'epoch': 56.61}

[INFO|2024-11-27 03:54:23] logging.py:157 >> {'loss': 0.1036, 'learning_rate': 4.9674e-05, 'epoch': 56.90}

[INFO|2024-11-27 03:56:05] logging.py:157 >> {'loss': 0.1264, 'learning_rate': 4.9670e-05, 'epoch': 57.20}

[INFO|2024-11-27 03:57:47] logging.py:157 >> {'loss': 0.1083, 'learning_rate': 4.9666e-05, 'epoch': 57.49}

[INFO|2024-11-27 03:59:30] logging.py:157 >> {'loss': 0.1112, 'learning_rate': 4.9662e-05, 'epoch': 57.78}

[INFO|2024-11-27 04:01:11] logging.py:157 >> {'loss': 0.1099, 'learning_rate': 4.9659e-05, 'epoch': 58.08}

[INFO|2024-11-27 04:02:58] logging.py:157 >> {'loss': 0.0958, 'learning_rate': 4.9655e-05, 'epoch': 58.37}

[INFO|2024-11-27 04:04:40] logging.py:157 >> {'loss': 0.0944, 'learning_rate': 4.9651e-05, 'epoch': 58.66}

[INFO|2024-11-27 04:04:40] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 04:04:40] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 04:04:40] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-27 04:04:50] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1000

[INFO|2024-11-27 04:04:50] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 04:04:50] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 04:04:50] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1000/tokenizer_config.json

[INFO|2024-11-27 04:04:50] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1000/special_tokens_map.json

[INFO|2024-11-27 04:06:29] logging.py:157 >> {'loss': 0.0953, 'learning_rate': 4.9647e-05, 'epoch': 58.96}

[INFO|2024-11-27 04:08:13] logging.py:157 >> {'loss': 0.1083, 'learning_rate': 4.9643e-05, 'epoch': 59.25}

[INFO|2024-11-27 04:09:52] logging.py:157 >> {'loss': 0.0880, 'learning_rate': 4.9639e-05, 'epoch': 59.54}

[INFO|2024-11-27 04:11:36] logging.py:157 >> {'loss': 0.0875, 'learning_rate': 4.9635e-05, 'epoch': 59.84}

[INFO|2024-11-27 04:13:23] logging.py:157 >> {'loss': 0.1165, 'learning_rate': 4.9631e-05, 'epoch': 60.13}

[INFO|2024-11-27 04:15:04] logging.py:157 >> {'loss': 0.0877, 'learning_rate': 4.9627e-05, 'epoch': 60.42}

[INFO|2024-11-27 04:16:44] logging.py:157 >> {'loss': 0.0814, 'learning_rate': 4.9623e-05, 'epoch': 60.71}

[INFO|2024-11-27 04:18:26] logging.py:157 >> {'loss': 0.0990, 'learning_rate': 4.9619e-05, 'epoch': 61.01}

[INFO|2024-11-27 04:20:09] logging.py:157 >> {'loss': 0.0750, 'learning_rate': 4.9615e-05, 'epoch': 61.30}

[INFO|2024-11-27 04:21:53] logging.py:157 >> {'loss': 0.0916, 'learning_rate': 4.9611e-05, 'epoch': 61.59}

[INFO|2024-11-27 04:23:34] logging.py:157 >> {'loss': 0.0716, 'learning_rate': 4.9607e-05, 'epoch': 61.89}

[INFO|2024-11-27 04:25:13] logging.py:157 >> {'loss': 0.0850, 'learning_rate': 4.9603e-05, 'epoch': 62.18}

[INFO|2024-11-27 04:26:58] logging.py:157 >> {'loss': 0.0734, 'learning_rate': 4.9599e-05, 'epoch': 62.47}

[INFO|2024-11-27 04:28:44] logging.py:157 >> {'loss': 0.0698, 'learning_rate': 4.9595e-05, 'epoch': 62.77}

[INFO|2024-11-27 04:30:29] logging.py:157 >> {'loss': 0.0851, 'learning_rate': 4.9590e-05, 'epoch': 63.06}

[INFO|2024-11-27 04:32:12] logging.py:157 >> {'loss': 0.0620, 'learning_rate': 4.9586e-05, 'epoch': 63.35}

[INFO|2024-11-27 04:33:55] logging.py:157 >> {'loss': 0.0665, 'learning_rate': 4.9582e-05, 'epoch': 63.65}

[INFO|2024-11-27 04:35:41] logging.py:157 >> {'loss': 0.0669, 'learning_rate': 4.9578e-05, 'epoch': 63.94}

[INFO|2024-11-27 04:37:20] logging.py:157 >> {'loss': 0.0736, 'learning_rate': 4.9574e-05, 'epoch': 64.23}

[INFO|2024-11-27 04:39:02] logging.py:157 >> {'loss': 0.0626, 'learning_rate': 4.9569e-05, 'epoch': 64.53}

[INFO|2024-11-27 04:39:02] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 04:39:02] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 04:39:02] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-27 04:39:13] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1100

[INFO|2024-11-27 04:39:13] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 04:39:13] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 04:39:13] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1100/tokenizer_config.json

[INFO|2024-11-27 04:39:13] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1100/special_tokens_map.json

[INFO|2024-11-27 04:40:56] logging.py:157 >> {'loss': 0.0591, 'learning_rate': 4.9565e-05, 'epoch': 64.82}

[INFO|2024-11-27 04:42:38] logging.py:157 >> {'loss': 0.0833, 'learning_rate': 4.9561e-05, 'epoch': 65.11}

[INFO|2024-11-27 04:44:24] logging.py:157 >> {'loss': 0.0536, 'learning_rate': 4.9556e-05, 'epoch': 65.41}

[INFO|2024-11-27 04:46:05] logging.py:157 >> {'loss': 0.0610, 'learning_rate': 4.9552e-05, 'epoch': 65.70}

[INFO|2024-11-27 04:47:49] logging.py:157 >> {'loss': 0.0620, 'learning_rate': 4.9548e-05, 'epoch': 65.99}

[INFO|2024-11-27 04:49:31] logging.py:157 >> {'loss': 0.0578, 'learning_rate': 4.9543e-05, 'epoch': 66.29}

[INFO|2024-11-27 04:51:14] logging.py:157 >> {'loss': 0.0550, 'learning_rate': 4.9539e-05, 'epoch': 66.58}

[INFO|2024-11-27 04:52:57] logging.py:157 >> {'loss': 0.0582, 'learning_rate': 4.9534e-05, 'epoch': 66.87}

[INFO|2024-11-27 04:54:39] logging.py:157 >> {'loss': 0.0590, 'learning_rate': 4.9530e-05, 'epoch': 67.17}

[INFO|2024-11-27 04:56:22] logging.py:157 >> {'loss': 0.0494, 'learning_rate': 4.9525e-05, 'epoch': 67.46}

[INFO|2024-11-27 04:58:06] logging.py:157 >> {'loss': 0.0540, 'learning_rate': 4.9521e-05, 'epoch': 67.75}

[INFO|2024-11-27 04:59:49] logging.py:157 >> {'loss': 0.0653, 'learning_rate': 4.9516e-05, 'epoch': 68.05}

[INFO|2024-11-27 05:01:32] logging.py:157 >> {'loss': 0.0477, 'learning_rate': 4.9512e-05, 'epoch': 68.34}

[INFO|2024-11-27 05:03:23] logging.py:157 >> {'loss': 0.0492, 'learning_rate': 4.9507e-05, 'epoch': 68.63}

[INFO|2024-11-27 05:05:09] logging.py:157 >> {'loss': 0.0548, 'learning_rate': 4.9502e-05, 'epoch': 68.93}

[INFO|2024-11-27 05:07:02] logging.py:157 >> {'loss': 0.0547, 'learning_rate': 4.9498e-05, 'epoch': 69.22}

[INFO|2024-11-27 05:08:45] logging.py:157 >> {'loss': 0.0460, 'learning_rate': 4.9493e-05, 'epoch': 69.51}

[INFO|2024-11-27 05:10:27] logging.py:157 >> {'loss': 0.0480, 'learning_rate': 4.9489e-05, 'epoch': 69.81}

[INFO|2024-11-27 05:12:10] logging.py:157 >> {'loss': 0.0574, 'learning_rate': 4.9484e-05, 'epoch': 70.10}

[INFO|2024-11-27 05:13:59] logging.py:157 >> {'loss': 0.0415, 'learning_rate': 4.9479e-05, 'epoch': 70.39}

[INFO|2024-11-27 05:13:59] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 05:13:59] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 05:13:59] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-27 05:14:10] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1200

[INFO|2024-11-27 05:14:10] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 05:14:10] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 05:14:10] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1200/tokenizer_config.json

[INFO|2024-11-27 05:14:10] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1200/special_tokens_map.json

[INFO|2024-11-27 05:15:59] logging.py:157 >> {'loss': 0.0437, 'learning_rate': 4.9474e-05, 'epoch': 70.69}

[INFO|2024-11-27 05:17:40] logging.py:157 >> {'loss': 0.0488, 'learning_rate': 4.9470e-05, 'epoch': 70.98}

[INFO|2024-11-27 05:19:22] logging.py:157 >> {'loss': 0.0446, 'learning_rate': 4.9465e-05, 'epoch': 71.27}

[INFO|2024-11-27 05:21:02] logging.py:157 >> {'loss': 0.0457, 'learning_rate': 4.9460e-05, 'epoch': 71.57}

[INFO|2024-11-27 05:22:46] logging.py:157 >> {'loss': 0.0456, 'learning_rate': 4.9455e-05, 'epoch': 71.86}

[INFO|2024-11-27 05:24:29] logging.py:157 >> {'loss': 0.0513, 'learning_rate': 4.9450e-05, 'epoch': 72.15}

[INFO|2024-11-27 05:26:10] logging.py:157 >> {'loss': 0.0413, 'learning_rate': 4.9446e-05, 'epoch': 72.45}

[INFO|2024-11-27 05:27:54] logging.py:157 >> {'loss': 0.0434, 'learning_rate': 4.9441e-05, 'epoch': 72.74}

[INFO|2024-11-27 05:29:37] logging.py:157 >> {'loss': 0.0464, 'learning_rate': 4.9436e-05, 'epoch': 73.03}

[INFO|2024-11-27 05:31:40] logging.py:157 >> {'loss': 0.0391, 'learning_rate': 4.9431e-05, 'epoch': 73.33}

[INFO|2024-11-27 05:33:31] logging.py:157 >> {'loss': 0.0427, 'learning_rate': 4.9426e-05, 'epoch': 73.62}

[INFO|2024-11-27 05:35:26] logging.py:157 >> {'loss': 0.0392, 'learning_rate': 4.9421e-05, 'epoch': 73.91}

[INFO|2024-11-27 05:37:12] logging.py:157 >> {'loss': 0.0462, 'learning_rate': 4.9416e-05, 'epoch': 74.21}

[INFO|2024-11-27 05:38:58] logging.py:157 >> {'loss': 0.0439, 'learning_rate': 4.9411e-05, 'epoch': 74.50}

[INFO|2024-11-27 05:40:40] logging.py:157 >> {'loss': 0.0405, 'learning_rate': 4.9406e-05, 'epoch': 74.79}

[INFO|2024-11-27 05:42:27] logging.py:157 >> {'loss': 0.0470, 'learning_rate': 4.9401e-05, 'epoch': 75.09}

[INFO|2024-11-27 05:44:10] logging.py:157 >> {'loss': 0.0361, 'learning_rate': 4.9396e-05, 'epoch': 75.38}

[INFO|2024-11-27 05:45:49] logging.py:157 >> {'loss': 0.0372, 'learning_rate': 4.9391e-05, 'epoch': 75.67}

[INFO|2024-11-27 05:47:34] logging.py:157 >> {'loss': 0.0401, 'learning_rate': 4.9386e-05, 'epoch': 75.97}

[INFO|2024-11-27 05:49:15] logging.py:157 >> {'loss': 0.0425, 'learning_rate': 4.9381e-05, 'epoch': 76.26}

[INFO|2024-11-27 05:49:15] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 05:49:15] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 05:49:15] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-27 05:49:25] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1300

[INFO|2024-11-27 05:49:25] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 05:49:25] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 05:49:25] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1300/tokenizer_config.json

[INFO|2024-11-27 05:49:25] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1300/special_tokens_map.json

[INFO|2024-11-27 05:51:06] logging.py:157 >> {'loss': 0.0362, 'learning_rate': 4.9375e-05, 'epoch': 76.55}

[INFO|2024-11-27 05:52:49] logging.py:157 >> {'loss': 0.0365, 'learning_rate': 4.9370e-05, 'epoch': 76.85}

[INFO|2024-11-27 05:54:36] logging.py:157 >> {'loss': 0.0414, 'learning_rate': 4.9365e-05, 'epoch': 77.14}

[INFO|2024-11-27 05:56:19] logging.py:157 >> {'loss': 0.0364, 'learning_rate': 4.9360e-05, 'epoch': 77.43}

[INFO|2024-11-27 05:57:58] logging.py:157 >> {'loss': 0.0333, 'learning_rate': 4.9355e-05, 'epoch': 77.73}

[INFO|2024-11-27 05:59:42] logging.py:157 >> {'loss': 0.0443, 'learning_rate': 4.9349e-05, 'epoch': 78.02}

[INFO|2024-11-27 06:01:21] logging.py:157 >> {'loss': 0.0309, 'learning_rate': 4.9344e-05, 'epoch': 78.31}

[INFO|2024-11-27 06:03:03] logging.py:157 >> {'loss': 0.0317, 'learning_rate': 4.9339e-05, 'epoch': 78.61}

[INFO|2024-11-27 06:04:47] logging.py:157 >> {'loss': 0.0326, 'learning_rate': 4.9333e-05, 'epoch': 78.90}

[INFO|2024-11-27 06:06:31] logging.py:157 >> {'loss': 0.0376, 'learning_rate': 4.9328e-05, 'epoch': 79.19}

[INFO|2024-11-27 06:08:15] logging.py:157 >> {'loss': 0.0313, 'learning_rate': 4.9323e-05, 'epoch': 79.49}

[INFO|2024-11-27 06:09:54] logging.py:157 >> {'loss': 0.0323, 'learning_rate': 4.9317e-05, 'epoch': 79.78}

[INFO|2024-11-27 06:11:36] logging.py:157 >> {'loss': 0.0395, 'learning_rate': 4.9312e-05, 'epoch': 80.07}

[INFO|2024-11-27 06:13:17] logging.py:157 >> {'loss': 0.0295, 'learning_rate': 4.9307e-05, 'epoch': 80.37}

[INFO|2024-11-27 06:15:01] logging.py:157 >> {'loss': 0.0310, 'learning_rate': 4.9301e-05, 'epoch': 80.66}

[INFO|2024-11-27 06:16:41] logging.py:157 >> {'loss': 0.0350, 'learning_rate': 4.9296e-05, 'epoch': 80.95}

[INFO|2024-11-27 06:18:22] logging.py:157 >> {'loss': 0.0332, 'learning_rate': 4.9290e-05, 'epoch': 81.25}

[INFO|2024-11-27 06:20:06] logging.py:157 >> {'loss': 0.0288, 'learning_rate': 4.9285e-05, 'epoch': 81.54}

[INFO|2024-11-27 06:21:48] logging.py:157 >> {'loss': 0.0297, 'learning_rate': 4.9279e-05, 'epoch': 81.83}

[INFO|2024-11-27 06:23:30] logging.py:157 >> {'loss': 0.0330, 'learning_rate': 4.9274e-05, 'epoch': 82.13}

[INFO|2024-11-27 06:23:30] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 06:23:30] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 06:23:30] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-27 06:23:41] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1400

[INFO|2024-11-27 06:23:41] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 06:23:41] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 06:23:41] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1400/tokenizer_config.json

[INFO|2024-11-27 06:23:41] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1400/special_tokens_map.json

[INFO|2024-11-27 06:25:24] logging.py:157 >> {'loss': 0.0268, 'learning_rate': 4.9268e-05, 'epoch': 82.42}

[INFO|2024-11-27 06:27:06] logging.py:157 >> {'loss': 0.0264, 'learning_rate': 4.9262e-05, 'epoch': 82.71}

[INFO|2024-11-27 06:28:44] logging.py:157 >> {'loss': 0.0376, 'learning_rate': 4.9257e-05, 'epoch': 83.01}

[INFO|2024-11-27 06:30:28] logging.py:157 >> {'loss': 0.0278, 'learning_rate': 4.9251e-05, 'epoch': 83.30}

[INFO|2024-11-27 06:32:07] logging.py:157 >> {'loss': 0.0277, 'learning_rate': 4.9245e-05, 'epoch': 83.59}

[INFO|2024-11-27 06:33:46] logging.py:157 >> {'loss': 0.0281, 'learning_rate': 4.9240e-05, 'epoch': 83.89}

[INFO|2024-11-27 06:35:30] logging.py:157 >> {'loss': 0.0316, 'learning_rate': 4.9234e-05, 'epoch': 84.18}

[INFO|2024-11-27 06:37:11] logging.py:157 >> {'loss': 0.0273, 'learning_rate': 4.9228e-05, 'epoch': 84.47}

[INFO|2024-11-27 06:38:53] logging.py:157 >> {'loss': 0.0296, 'learning_rate': 4.9223e-05, 'epoch': 84.77}

[INFO|2024-11-27 06:40:36] logging.py:157 >> {'loss': 0.0363, 'learning_rate': 4.9217e-05, 'epoch': 85.06}

[INFO|2024-11-27 06:42:23] logging.py:157 >> {'loss': 0.0279, 'learning_rate': 4.9211e-05, 'epoch': 85.35}

[INFO|2024-11-27 06:44:03] logging.py:157 >> {'loss': 0.0276, 'learning_rate': 4.9205e-05, 'epoch': 85.65}

[INFO|2024-11-27 06:45:43] logging.py:157 >> {'loss': 0.0287, 'learning_rate': 4.9199e-05, 'epoch': 85.94}

[INFO|2024-11-27 06:47:24] logging.py:157 >> {'loss': 0.0352, 'learning_rate': 4.9194e-05, 'epoch': 86.23}

[INFO|2024-11-27 06:49:04] logging.py:157 >> {'loss': 0.0267, 'learning_rate': 4.9188e-05, 'epoch': 86.53}

[INFO|2024-11-27 06:50:48] logging.py:157 >> {'loss': 0.0287, 'learning_rate': 4.9182e-05, 'epoch': 86.82}

[INFO|2024-11-27 06:52:30] logging.py:157 >> {'loss': 0.0321, 'learning_rate': 4.9176e-05, 'epoch': 87.11}

[INFO|2024-11-27 06:54:08] logging.py:157 >> {'loss': 0.0253, 'learning_rate': 4.9170e-05, 'epoch': 87.41}

[INFO|2024-11-27 06:55:48] logging.py:157 >> {'loss': 0.0272, 'learning_rate': 4.9164e-05, 'epoch': 87.70}

[INFO|2024-11-27 06:57:36] logging.py:157 >> {'loss': 0.0271, 'learning_rate': 4.9158e-05, 'epoch': 87.99}

[INFO|2024-11-27 06:57:36] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 06:57:36] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 06:57:36] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-27 06:57:49] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1500

[INFO|2024-11-27 06:57:49] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 06:57:49] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 06:57:49] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1500/tokenizer_config.json

[INFO|2024-11-27 06:57:49] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1500/special_tokens_map.json

[INFO|2024-11-27 06:59:42] logging.py:157 >> {'loss': 0.0290, 'learning_rate': 4.9152e-05, 'epoch': 88.29}

[INFO|2024-11-27 07:01:26] logging.py:157 >> {'loss': 0.0238, 'learning_rate': 4.9146e-05, 'epoch': 88.58}

[INFO|2024-11-27 07:03:10] logging.py:157 >> {'loss': 0.0254, 'learning_rate': 4.9140e-05, 'epoch': 88.87}

[INFO|2024-11-27 07:04:58] logging.py:157 >> {'loss': 0.0276, 'learning_rate': 4.9134e-05, 'epoch': 89.17}

[INFO|2024-11-27 07:06:38] logging.py:157 >> {'loss': 0.0236, 'learning_rate': 4.9128e-05, 'epoch': 89.46}

[INFO|2024-11-27 07:08:21] logging.py:157 >> {'loss': 0.0237, 'learning_rate': 4.9122e-05, 'epoch': 89.75}

[INFO|2024-11-27 07:10:00] logging.py:157 >> {'loss': 0.0301, 'learning_rate': 4.9116e-05, 'epoch': 90.05}

[INFO|2024-11-27 07:11:41] logging.py:157 >> {'loss': 0.0229, 'learning_rate': 4.9110e-05, 'epoch': 90.34}

[INFO|2024-11-27 07:13:21] logging.py:157 >> {'loss': 0.0234, 'learning_rate': 4.9103e-05, 'epoch': 90.63}

[INFO|2024-11-27 07:15:02] logging.py:157 >> {'loss': 0.0237, 'learning_rate': 4.9097e-05, 'epoch': 90.93}

[INFO|2024-11-27 07:16:40] logging.py:157 >> {'loss': 0.0333, 'learning_rate': 4.9091e-05, 'epoch': 91.22}

[INFO|2024-11-27 07:18:25] logging.py:157 >> {'loss': 0.0234, 'learning_rate': 4.9085e-05, 'epoch': 91.51}

[INFO|2024-11-27 07:20:07] logging.py:157 >> {'loss': 0.0244, 'learning_rate': 4.9079e-05, 'epoch': 91.81}

[INFO|2024-11-27 07:22:01] logging.py:157 >> {'loss': 0.0242, 'learning_rate': 4.9072e-05, 'epoch': 92.10}

[INFO|2024-11-27 07:23:43] logging.py:157 >> {'loss': 0.0211, 'learning_rate': 4.9066e-05, 'epoch': 92.39}

[INFO|2024-11-27 07:25:27] logging.py:157 >> {'loss': 0.0231, 'learning_rate': 4.9060e-05, 'epoch': 92.69}

[INFO|2024-11-27 07:27:06] logging.py:157 >> {'loss': 0.0228, 'learning_rate': 4.9053e-05, 'epoch': 92.98}

[INFO|2024-11-27 07:28:48] logging.py:157 >> {'loss': 0.0241, 'learning_rate': 4.9047e-05, 'epoch': 93.27}

[INFO|2024-11-27 07:30:29] logging.py:157 >> {'loss': 0.0206, 'learning_rate': 4.9041e-05, 'epoch': 93.57}

[INFO|2024-11-27 07:32:19] logging.py:157 >> {'loss': 0.0220, 'learning_rate': 4.9034e-05, 'epoch': 93.86}

[INFO|2024-11-27 07:32:19] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 07:32:19] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 07:32:19] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-27 07:32:32] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1600

[INFO|2024-11-27 07:32:32] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 07:32:32] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 07:32:32] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1600/tokenizer_config.json

[INFO|2024-11-27 07:32:32] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1600/special_tokens_map.json

[INFO|2024-11-27 07:34:20] logging.py:157 >> {'loss': 0.0263, 'learning_rate': 4.9028e-05, 'epoch': 94.15}

[INFO|2024-11-27 07:35:58] logging.py:157 >> {'loss': 0.0198, 'learning_rate': 4.9022e-05, 'epoch': 94.45}

[INFO|2024-11-27 07:37:42] logging.py:157 >> {'loss': 0.0216, 'learning_rate': 4.9015e-05, 'epoch': 94.74}

[INFO|2024-11-27 07:39:26] logging.py:157 >> {'loss': 0.0250, 'learning_rate': 4.9009e-05, 'epoch': 95.03}

[INFO|2024-11-27 07:41:12] logging.py:157 >> {'loss': 0.0192, 'learning_rate': 4.9002e-05, 'epoch': 95.33}

[INFO|2024-11-27 07:42:54] logging.py:157 >> {'loss': 0.0193, 'learning_rate': 4.8996e-05, 'epoch': 95.62}

[INFO|2024-11-27 07:44:31] logging.py:157 >> {'loss': 0.0207, 'learning_rate': 4.8989e-05, 'epoch': 95.91}

[INFO|2024-11-27 07:46:11] logging.py:157 >> {'loss': 0.0250, 'learning_rate': 4.8983e-05, 'epoch': 96.21}

[INFO|2024-11-27 07:47:52] logging.py:157 >> {'loss': 0.0197, 'learning_rate': 4.8976e-05, 'epoch': 96.50}

[INFO|2024-11-27 07:49:30] logging.py:157 >> {'loss': 0.0204, 'learning_rate': 4.8969e-05, 'epoch': 96.79}

[INFO|2024-11-27 07:51:10] logging.py:157 >> {'loss': 0.0216, 'learning_rate': 4.8963e-05, 'epoch': 97.09}

[INFO|2024-11-27 07:52:51] logging.py:157 >> {'loss': 0.0185, 'learning_rate': 4.8956e-05, 'epoch': 97.38}

[INFO|2024-11-27 07:54:33] logging.py:157 >> {'loss': 0.0192, 'learning_rate': 4.8949e-05, 'epoch': 97.67}

[INFO|2024-11-27 07:56:12] logging.py:157 >> {'loss': 0.0198, 'learning_rate': 4.8943e-05, 'epoch': 97.97}

[INFO|2024-11-27 07:57:55] logging.py:157 >> {'loss': 0.0220, 'learning_rate': 4.8936e-05, 'epoch': 98.26}

[INFO|2024-11-27 07:59:36] logging.py:157 >> {'loss': 0.0172, 'learning_rate': 4.8929e-05, 'epoch': 98.55}

[INFO|2024-11-27 08:01:21] logging.py:157 >> {'loss': 0.0185, 'learning_rate': 4.8923e-05, 'epoch': 98.85}

[INFO|2024-11-27 08:03:01] logging.py:157 >> {'loss': 0.0245, 'learning_rate': 4.8916e-05, 'epoch': 99.14}

[INFO|2024-11-27 08:04:40] logging.py:157 >> {'loss': 0.0176, 'learning_rate': 4.8909e-05, 'epoch': 99.43}

[INFO|2024-11-27 08:06:30] logging.py:157 >> {'loss': 0.0188, 'learning_rate': 4.8902e-05, 'epoch': 99.73}

[INFO|2024-11-27 08:06:30] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 08:06:30] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 08:06:30] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-27 08:06:41] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1700

[INFO|2024-11-27 08:06:41] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 08:06:41] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 08:06:41] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1700/tokenizer_config.json

[INFO|2024-11-27 08:06:41] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1700/special_tokens_map.json

[INFO|2024-11-27 08:08:20] logging.py:157 >> {'loss': 0.0225, 'learning_rate': 4.8896e-05, 'epoch': 100.02}

[INFO|2024-11-27 08:10:02] logging.py:157 >> {'loss': 0.0170, 'learning_rate': 4.8889e-05, 'epoch': 100.31}

[INFO|2024-11-27 08:11:45] logging.py:157 >> {'loss': 0.0189, 'learning_rate': 4.8882e-05, 'epoch': 100.60}

[INFO|2024-11-27 08:13:25] logging.py:157 >> {'loss': 0.0182, 'learning_rate': 4.8875e-05, 'epoch': 100.90}

[INFO|2024-11-27 08:15:10] logging.py:157 >> {'loss': 0.0239, 'learning_rate': 4.8868e-05, 'epoch': 101.19}

[INFO|2024-11-27 08:16:48] logging.py:157 >> {'loss': 0.0170, 'learning_rate': 4.8861e-05, 'epoch': 101.48}

[INFO|2024-11-27 08:18:31] logging.py:157 >> {'loss': 0.0193, 'learning_rate': 4.8854e-05, 'epoch': 101.78}

[INFO|2024-11-27 08:20:12] logging.py:157 >> {'loss': 0.0213, 'learning_rate': 4.8847e-05, 'epoch': 102.07}

[INFO|2024-11-27 08:21:55] logging.py:157 >> {'loss': 0.0183, 'learning_rate': 4.8840e-05, 'epoch': 102.36}

[INFO|2024-11-27 08:23:44] logging.py:157 >> {'loss': 0.0191, 'learning_rate': 4.8833e-05, 'epoch': 102.66}

[INFO|2024-11-27 08:25:24] logging.py:157 >> {'loss': 0.0196, 'learning_rate': 4.8826e-05, 'epoch': 102.95}

[INFO|2024-11-27 08:27:06] logging.py:157 >> {'loss': 0.0208, 'learning_rate': 4.8819e-05, 'epoch': 103.24}

[INFO|2024-11-27 08:28:51] logging.py:157 >> {'loss': 0.0172, 'learning_rate': 4.8812e-05, 'epoch': 103.54}

[INFO|2024-11-27 08:30:32] logging.py:157 >> {'loss': 0.0182, 'learning_rate': 4.8805e-05, 'epoch': 103.83}

[INFO|2024-11-27 08:32:09] logging.py:157 >> {'loss': 0.0212, 'learning_rate': 4.8798e-05, 'epoch': 104.12}

[INFO|2024-11-27 08:33:53] logging.py:157 >> {'loss': 0.0177, 'learning_rate': 4.8791e-05, 'epoch': 104.42}

[INFO|2024-11-27 08:35:34] logging.py:157 >> {'loss': 0.0166, 'learning_rate': 4.8784e-05, 'epoch': 104.71}

[INFO|2024-11-27 08:37:15] logging.py:157 >> {'loss': 0.0193, 'learning_rate': 4.8776e-05, 'epoch': 105.00}

[INFO|2024-11-27 08:38:58] logging.py:157 >> {'loss': 0.0146, 'learning_rate': 4.8769e-05, 'epoch': 105.30}

[INFO|2024-11-27 08:40:39] logging.py:157 >> {'loss': 0.0144, 'learning_rate': 4.8762e-05, 'epoch': 105.59}

[INFO|2024-11-27 08:40:39] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 08:40:39] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 08:40:39] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-27 08:40:49] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1800

[INFO|2024-11-27 08:40:49] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 08:40:49] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 08:40:49] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1800/tokenizer_config.json

[INFO|2024-11-27 08:40:49] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1800/special_tokens_map.json

[INFO|2024-11-27 08:42:28] logging.py:157 >> {'loss': 0.0154, 'learning_rate': 4.8755e-05, 'epoch': 105.88}

[INFO|2024-11-27 08:44:11] logging.py:157 >> {'loss': 0.0194, 'learning_rate': 4.8748e-05, 'epoch': 106.18}

[INFO|2024-11-27 08:45:54] logging.py:157 >> {'loss': 0.0147, 'learning_rate': 4.8740e-05, 'epoch': 106.47}

[INFO|2024-11-27 08:47:35] logging.py:157 >> {'loss': 0.0135, 'learning_rate': 4.8733e-05, 'epoch': 106.76}

[INFO|2024-11-27 08:49:21] logging.py:157 >> {'loss': 0.0176, 'learning_rate': 4.8726e-05, 'epoch': 107.06}

[INFO|2024-11-27 08:51:02] logging.py:157 >> {'loss': 0.0140, 'learning_rate': 4.8718e-05, 'epoch': 107.35}

[INFO|2024-11-27 08:52:42] logging.py:157 >> {'loss': 0.0136, 'learning_rate': 4.8711e-05, 'epoch': 107.64}

[INFO|2024-11-27 08:54:23] logging.py:157 >> {'loss': 0.0137, 'learning_rate': 4.8704e-05, 'epoch': 107.94}

[INFO|2024-11-27 08:56:04] logging.py:157 >> {'loss': 0.0159, 'learning_rate': 4.8696e-05, 'epoch': 108.23}

[INFO|2024-11-27 08:57:49] logging.py:157 >> {'loss': 0.0126, 'learning_rate': 4.8689e-05, 'epoch': 108.52}

[INFO|2024-11-27 08:59:29] logging.py:157 >> {'loss': 0.0117, 'learning_rate': 4.8681e-05, 'epoch': 108.82}

[INFO|2024-11-27 09:01:12] logging.py:157 >> {'loss': 0.0147, 'learning_rate': 4.8674e-05, 'epoch': 109.11}

[INFO|2024-11-27 09:02:52] logging.py:157 >> {'loss': 0.0123, 'learning_rate': 4.8666e-05, 'epoch': 109.40}

[INFO|2024-11-27 09:04:35] logging.py:157 >> {'loss': 0.0121, 'learning_rate': 4.8659e-05, 'epoch': 109.70}

[INFO|2024-11-27 09:06:14] logging.py:157 >> {'loss': 0.0120, 'learning_rate': 4.8651e-05, 'epoch': 109.99}

[INFO|2024-11-27 09:07:59] logging.py:157 >> {'loss': 0.0142, 'learning_rate': 4.8644e-05, 'epoch': 110.28}

[INFO|2024-11-27 09:09:39] logging.py:157 >> {'loss': 0.0130, 'learning_rate': 4.8636e-05, 'epoch': 110.58}

[INFO|2024-11-27 09:11:20] logging.py:157 >> {'loss': 0.0123, 'learning_rate': 4.8629e-05, 'epoch': 110.87}

[INFO|2024-11-27 09:13:02] logging.py:157 >> {'loss': 0.0158, 'learning_rate': 4.8621e-05, 'epoch': 111.16}

[INFO|2024-11-27 09:14:46] logging.py:157 >> {'loss': 0.0129, 'learning_rate': 4.8613e-05, 'epoch': 111.46}

[INFO|2024-11-27 09:14:46] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 09:14:46] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 09:14:46] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-27 09:14:57] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1900

[INFO|2024-11-27 09:14:57] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 09:14:57] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 09:14:57] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1900/tokenizer_config.json

[INFO|2024-11-27 09:14:57] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-1900/special_tokens_map.json

[INFO|2024-11-27 09:16:35] logging.py:157 >> {'loss': 0.0118, 'learning_rate': 4.8606e-05, 'epoch': 111.75}

[INFO|2024-11-27 09:18:16] logging.py:157 >> {'loss': 0.0162, 'learning_rate': 4.8598e-05, 'epoch': 112.04}

[INFO|2024-11-27 09:19:58] logging.py:157 >> {'loss': 0.0116, 'learning_rate': 4.8590e-05, 'epoch': 112.34}

[INFO|2024-11-27 09:21:40] logging.py:157 >> {'loss': 0.0124, 'learning_rate': 4.8583e-05, 'epoch': 112.63}

[INFO|2024-11-27 09:23:25] logging.py:157 >> {'loss': 0.0118, 'learning_rate': 4.8575e-05, 'epoch': 112.92}

[INFO|2024-11-27 09:25:08] logging.py:157 >> {'loss': 0.0130, 'learning_rate': 4.8567e-05, 'epoch': 113.22}

[INFO|2024-11-27 09:26:55] logging.py:157 >> {'loss': 0.0111, 'learning_rate': 4.8560e-05, 'epoch': 113.51}

[INFO|2024-11-27 09:28:35] logging.py:157 >> {'loss': 0.0118, 'learning_rate': 4.8552e-05, 'epoch': 113.80}

[INFO|2024-11-27 09:30:13] logging.py:157 >> {'loss': 0.0123, 'learning_rate': 4.8544e-05, 'epoch': 114.10}

[INFO|2024-11-27 09:31:56] logging.py:157 >> {'loss': 0.0099, 'learning_rate': 4.8536e-05, 'epoch': 114.39}

[INFO|2024-11-27 09:33:38] logging.py:157 >> {'loss': 0.0111, 'learning_rate': 4.8528e-05, 'epoch': 114.68}

[INFO|2024-11-27 09:35:23] logging.py:157 >> {'loss': 0.0106, 'learning_rate': 4.8520e-05, 'epoch': 114.98}

[INFO|2024-11-27 09:37:05] logging.py:157 >> {'loss': 0.0112, 'learning_rate': 4.8513e-05, 'epoch': 115.27}

[INFO|2024-11-27 09:38:45] logging.py:157 >> {'loss': 0.0094, 'learning_rate': 4.8505e-05, 'epoch': 115.56}

[INFO|2024-11-27 09:40:32] logging.py:157 >> {'loss': 0.0093, 'learning_rate': 4.8497e-05, 'epoch': 115.86}

[INFO|2024-11-27 09:42:14] logging.py:157 >> {'loss': 0.0116, 'learning_rate': 4.8489e-05, 'epoch': 116.15}

[INFO|2024-11-27 09:43:52] logging.py:157 >> {'loss': 0.0089, 'learning_rate': 4.8481e-05, 'epoch': 116.44}

[INFO|2024-11-27 09:45:37] logging.py:157 >> {'loss': 0.0095, 'learning_rate': 4.8473e-05, 'epoch': 116.74}

[INFO|2024-11-27 09:47:15] logging.py:157 >> {'loss': 0.0125, 'learning_rate': 4.8465e-05, 'epoch': 117.03}

[INFO|2024-11-27 09:49:00] logging.py:157 >> {'loss': 0.0086, 'learning_rate': 4.8457e-05, 'epoch': 117.32}

[INFO|2024-11-27 09:49:00] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 09:49:00] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 09:49:00] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-27 09:49:10] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-2000

[INFO|2024-11-27 09:49:10] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 09:49:10] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 09:49:10] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-2000/tokenizer_config.json

[INFO|2024-11-27 09:49:10] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-2000/special_tokens_map.json

[INFO|2024-11-27 09:50:53] logging.py:157 >> {'loss': 0.0080, 'learning_rate': 4.8449e-05, 'epoch': 117.62}

[INFO|2024-11-27 09:52:34] logging.py:157 >> {'loss': 0.0103, 'learning_rate': 4.8441e-05, 'epoch': 117.91}

[INFO|2024-11-27 09:54:14] logging.py:157 >> {'loss': 0.0098, 'learning_rate': 4.8433e-05, 'epoch': 118.20}

[INFO|2024-11-27 09:55:56] logging.py:157 >> {'loss': 0.0086, 'learning_rate': 4.8424e-05, 'epoch': 118.50}

[INFO|2024-11-27 09:57:36] logging.py:157 >> {'loss': 0.0084, 'learning_rate': 4.8416e-05, 'epoch': 118.79}

[INFO|2024-11-27 09:59:18] logging.py:157 >> {'loss': 0.0089, 'learning_rate': 4.8408e-05, 'epoch': 119.08}

[INFO|2024-11-27 10:00:57] logging.py:157 >> {'loss': 0.0083, 'learning_rate': 4.8400e-05, 'epoch': 119.38}

[INFO|2024-11-27 10:02:43] logging.py:157 >> {'loss': 0.0082, 'learning_rate': 4.8392e-05, 'epoch': 119.67}

[INFO|2024-11-27 10:04:25] logging.py:157 >> {'loss': 0.0078, 'learning_rate': 4.8384e-05, 'epoch': 119.96}

[INFO|2024-11-27 10:06:15] logging.py:157 >> {'loss': 0.0086, 'learning_rate': 4.8375e-05, 'epoch': 120.26}

[INFO|2024-11-27 10:07:55] logging.py:157 >> {'loss': 0.0070, 'learning_rate': 4.8367e-05, 'epoch': 120.55}

[INFO|2024-11-27 10:09:35] logging.py:157 >> {'loss': 0.0078, 'learning_rate': 4.8359e-05, 'epoch': 120.84}

[INFO|2024-11-27 10:11:15] logging.py:157 >> {'loss': 0.0081, 'learning_rate': 4.8351e-05, 'epoch': 121.14}

[INFO|2024-11-27 10:13:01] logging.py:157 >> {'loss': 0.0066, 'learning_rate': 4.8342e-05, 'epoch': 121.43}

[INFO|2024-11-27 10:14:40] logging.py:157 >> {'loss': 0.0061, 'learning_rate': 4.8334e-05, 'epoch': 121.72}

[INFO|2024-11-27 10:16:21] logging.py:157 >> {'loss': 0.0079, 'learning_rate': 4.8326e-05, 'epoch': 122.02}

[INFO|2024-11-27 10:18:04] logging.py:157 >> {'loss': 0.0067, 'learning_rate': 4.8317e-05, 'epoch': 122.31}

[INFO|2024-11-27 10:19:51] logging.py:157 >> {'loss': 0.0064, 'learning_rate': 4.8309e-05, 'epoch': 122.60}

[INFO|2024-11-27 10:21:35] logging.py:157 >> {'loss': 0.0066, 'learning_rate': 4.8300e-05, 'epoch': 122.90}

[INFO|2024-11-27 10:23:21] logging.py:157 >> {'loss': 0.0086, 'learning_rate': 4.8292e-05, 'epoch': 123.19}

[INFO|2024-11-27 10:23:21] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 10:23:21] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 10:23:21] trainer.py:4122 >>   Batch size = 1

[INFO|2024-11-27 10:23:32] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-2100

[INFO|2024-11-27 10:23:32] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 10:23:32] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 10:23:32] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-2100/tokenizer_config.json

[INFO|2024-11-27 10:23:32] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/checkpoint-2100/special_tokens_map.json

[INFO|2024-11-27 10:25:27] logging.py:157 >> {'loss': 0.0067, 'learning_rate': 4.8284e-05, 'epoch': 123.48}

[INFO|2024-11-27 10:26:28] trainer.py:2584 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)



[INFO|2024-11-27 10:26:28] trainer.py:3801 >> Saving model checkpoint to saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47

[INFO|2024-11-27 10:26:28] configuration_utils.py:677 >> loading configuration file /root/autodl-tmp/deepseek-llm-7b-chat/config.json

[INFO|2024-11-27 10:26:28] configuration_utils.py:746 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 102400
}


[INFO|2024-11-27 10:26:28] tokenization_utils_base.py:2646 >> tokenizer config file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/tokenizer_config.json

[INFO|2024-11-27 10:26:28] tokenization_utils_base.py:2655 >> Special tokens file saved in saves/DeepSeek-LLM-7B-Chat/lora/train_2024-11-26-17-59-47/special_tokens_map.json

[WARNING|2024-11-27 10:26:29] logging.py:162 >> No metric eval_accuracy to plot.

[INFO|2024-11-27 10:26:29] trainer.py:4117 >> 
***** Running Evaluation *****

[INFO|2024-11-27 10:26:29] trainer.py:4119 >>   Num examples = 243

[INFO|2024-11-27 10:26:29] trainer.py:4122 >>   Batch size = 1

